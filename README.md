# Pipeline de Previs√µes ‚Äî README

Este reposit√≥rio cont√©m uma pipeline escal√°vel para gerar previs√µes semanais por **loja √ó produto** combinando tr√™s abordagens:

1. **Zeros** para pares **descontinuados** (sem venda h√° X semanas);
2. **LightGBM** nos **pares com maior relev√¢ncia** (top N por volume);
3. **M√©dias r√°pidas** para o **restante** (com fallback global).

O fluxo produz **arquivos parquet de checkpoint por *chunk* de lojas** e, ao final, um **parquet √∫nico**. H√° tamb√©m utilit√°rios para **limpar CSVs** de checkpoints antigos e **mesclar/renomear/arredundar** as previs√µes finais.

---

## üì¶ Estrutura dos scripts

### 1) `solver.py` ‚Äî motor da pipeline (gradiente de qualidade)

**O que faz, passo a passo:**

1. **L√™** os tr√™s parquets de entrada:

   * `TRANSACTIONS_PARQUET`: transa√ß√µes com `transaction_date`, `quantity`, `discount`, `internal_store_id`, `internal_product_id`.
   * `PDVS_PARQUET`: metadados de lojas (usa `pdv` se existir; sen√£o autodetecta).
   * `PRODUCTS_PARQUET`: metadados de produtos (usa `produto` se existir; sen√£o autodetecta).
2. **Preprocessa** transa√ß√µes (datas, tipos num√©ricos, coluna `week_start` alinhada √† segunda-feira).
3. **Agrega semanalmente** por `(loja, produto, week_start)` com features: `quantity`, `trans_count`, `discount_mean`, `discount_max`.
4. **Pr√©-calcula** a **m√©dia das √∫ltimas 4 semanas por produto (global)**: Series para **lookup O(1)**.
5. **Identifica pares extintos** (semana atual ‚àí √∫ltima semana de venda > `WEEKS_THRESHOLD`) ‚áí **previs√£o 0** em todos os horizontes.
6. **Amostra produtos ‚Äúrepresentativos‚Äù** (por `PRODUCT_SAMPLE_RATE`), para tentar usar **m√©dia 4 semanas por loja√óproduto** (se existir hist√≥rico recente, sen√£o cai no global).
7. **(Opcional, se `lightgbm` dispon√≠vel)**: seleciona **TOP\_LGBM\_PAIRS** (maiores volumes), **constr√≥i painel**, gera **lags/rolagens/agregados**, prepara **targets por horizonte** e **treina 1 modelo por horizonte** com *early stopping*.

   * Em seguida, **prediz** no **√∫ltimo ponto** de cada par top e **sobrescreve** a predi√ß√£o base (a menos que o par seja extinto).
8. **Calcula m√©tricas baseline (WMAPE)** por horizonte (m√©dia m√≥vel) e salva em `output_forecast/metrics/metrics_by_horizon.csv`.
9. **Gera previs√µes por *chunk* de lojas**:

   * Para cada loja no chunk, monta predi√ß√£o **vetorizada** nos 5 horizontes para **todos os produtos**:

     * **Extintos** ‚Üí 0;
     * **Representativos** ‚Üí m√©dia 4 semanas **loja√óproduto** se existir, **fallback** global por produto;
     * **Demais** ‚Üí m√©dia 4 semanas **global** por produto;
     * **Top LGBM** (quando aplic√°vel) ‚Üí **override** com a sa√≠da do modelo.
   * Escreve **um parquet por chunk** em `output_forecast/predictions_tmp/chunk_XXXXX.parquet`.
10. **Concatena** todos os chunks parquet em **um arquivo final**:

    * `output_forecast/full_dataset_predictions.parquet`
    * **Obs.:** a vers√£o atual do `solver.py` tamb√©m grava `full_dataset_predictions.csv`. Se **n√£o quiser CSV final**, remova a chamada ao `COPY ... TO ... (HEADER, DELIMITER ',')` em `finalize_concatenation`.

**Sa√≠da principal:**

* `output_forecast/predictions_tmp/chunk_*.parquet` (checkpoints por chunk);
* `output_forecast/full_dataset_predictions.parquet` (todas as lojas √ó todos os produtos √ó 5 horizontes);
* `output_forecast/metrics/metrics_by_horizon.csv` (baseline de refer√™ncia).

**Campos das previs√µes:**

* `internal_store_id`, `internal_product_id`, `horizon` (1..5), `prediction` (float ‚â• 0).

---

### 2) `rm_csvs.py` ‚Äî limpeza dos CSVs de checkpoint

**O que faz:** apaga **todos os `.csv`** em `output_forecast/predictions_tmp`.

**Quando usar:**

* Se voc√™ alterou o `solver.py` para **n√£o gravar CSV por chunk**, ou
* Se restaram CSVs legados e voc√™ quer manter apenas os parquets de checkpoint.

> ‚ö†Ô∏è Observa√ß√£o importante: no `solver.py` atual, o crit√©rio de ‚Äúpular chunk j√° processado‚Äù checa se **parquet E csv existem**. Se voc√™ **n√£o grava CSVs**, ou se rodar `rm_csvs.py` **antes de retomar**, o solver **n√£o vai pular** chunks j√° processados (ele vai reprocessar e **sobrescrever** o mesmo parquet).
> Se deseja permitir **resume** apenas com parquet, ajuste no `solver.py` a verifica√ß√£o para olhar **s√≥ o parquet**.

---

### 3) `concat.py` ‚Äî mescla final com arredondamento e renomea√ß√£o

**O que faz:**

* L√™ **todos os parquets** de `output_forecast/predictions_tmp/`;
* **Arredonda** `prediction` para inteiro (`ROUND ‚Üí BIGINT`);
* **Renomeia** campos para layout final:

  * `internal_store_id ‚Üí pdv`
  * `internal_product_id ‚Üí produto`
  * `horizon ‚Üí semana`
  * `prediction ‚Üí quantidade` (inteiro)
* Escreve **um √∫nico parquet**:
  `output_forecast/predictions_tmp/merged_predictions_int.parquet`

**Quando usar:**

* Quando voc√™ precisa de um **arquivo final ‚Äúbonito‚Äù**, com campo de quantidade **inteira** e nomes ‚Äúpdv/produto/semana/quantidade‚Äù.
* Normalmente rodado **depois** do `solver.py` (e, se quiser, **depois** do `rm_csvs.py`).

---

## üóÇÔ∏è Estrutura de diret√≥rios esperada

```
output_forecast/
‚îú‚îÄ‚îÄ predictions_tmp/            # checkpoints por chunk (parquet) e arquivos auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ chunk_00000.parquet
‚îÇ   ‚îú‚îÄ‚îÄ chunk_00001.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îî‚îÄ‚îÄ metrics_by_horizon.csv  # baseline WMAPE por horizonte
‚îú‚îÄ‚îÄ full_dataset_predictions.parquet   # concat final (solver)
‚îî‚îÄ‚îÄ full_dataset_predictions.csv       # concat final CSV (opcional, solver)
```

> Se voc√™ **n√£o quiser CSV algum**, ajuste o `solver.py` para n√£o chamar o `COPY ... TO ... CSV` no final **e** n√£o gravar CSV por chunk.

---

## ‚öôÔ∏è Par√¢metros principais (editar em `solver.py`)

* **Caminhos de entrada**

  * `PDVS_PARQUET`, `TRANSACTIONS_PARQUET`, `PRODUCTS_PARQUET`

* **Qualidade √ó Desempenho**

  * `TOP_LGBM_PAIRS` (ex.: `50_000`): n¬∫ de pares (loja√óproduto) para LightGBM;
  * `MAX_LGBM_PANEL_ROWS` (ex.: `4_000_000`): teto de linhas do painel de treino;
  * `LGBM_MIN_TRAIN_ROWS`: m√≠nimo de linhas por horizonte para treinar o modelo;
  * `PRODUCT_SAMPLE_RATE` (ex.: `0.20`): % de produtos ‚Äúrepresentativos‚Äù que usam m√©dia loja√óproduto;
  * `WEEKS_THRESHOLD` (ex.: `12`): sem venda acima desse n¬∫ de semanas ‚áí **extinto** (predi√ß√£o 0);
  * `STORE_CHUNK_SIZE` (ex.: `200`): n¬∫ de lojas por chunk (checkpoint); ajuste conforme RAM/IO.

* **Outros**

  * `HORIZONS = [1,2,3,4,5]`
  * `SEED = 42` (reprodutibilidade)
  * Diret√≥rios: `OUTDIR`, `TMPDIR`, `METRICS_DIR`

---

## üß™ Pr√©-requisitos e instala√ß√£o

Python ‚â• 3.9. Instale as depend√™ncias (LightGBM √© opcional ‚Äî o script funciona sem ele):

```bash
python -m pip install --upgrade pip
python -m pip install duckdb pandas numpy pyarrow tqdm
# opcional (para o bloco LightGBM):
python -m pip install lightgbm
```

> Se preferir, use tamb√©m `fastparquet` como engine parquet alternativa.

---

## ‚ñ∂Ô∏è Como rodar

1. **Rodar a pipeline principal (gera os chunks e o parquet final):**

```bash
python solver.py
```

2. **(Opcional) Remover CSVs de checkpoints (se houver):**

```bash
python rm_csvs.py
```

3. **Mesclar e arredondar (sa√≠da com nomes finais e quantidade inteira):**

```bash
python concat.py
```

> Se quiser **pular** a concatena√ß√£o final do `solver.py` para controlar tudo por fora, comente a chamada `finalize_concatenation` no final do `solver.py` e use s√≥ o `concat.py`.

---

## üß† L√≥gica de previs√£o (‚Äúgradiente de qualidade‚Äù)

* **Extintos** (`weeks_since_sale > WEEKS_THRESHOLD`):
  ‚Üí `prediction = 0` para todos os horizontes.

* **Top pares (LightGBM)**:

  1. Seleciona TOP por `total_sales` (com *cap* por n¬∫ de linhas de painel);
  2. Cria **lags** (`1,2,3,4,8,12,52`), **rolagens** (`qty_4w_sum`, `qty_12w_mean`) e **agregados semanais** (`store_week_total`, `product_global_week`);
  3. Prepara **targets `target_h{h}`** e treina **1 modelo por horizonte** com *early stopping*;
  4. **Override** das previs√µes base usando a **predi√ß√£o do √∫ltimo ponto** de cada par.

* **Demais pares**:

  * **Produtos representativos**: m√©dia 4s **loja√óproduto** (se houver hist√≥rico recente), **fallback** m√©dia 4s **global por produto**;
  * **Outros**: m√©dia 4s **global por produto**.

Tudo √© **vetorizado** por loja, evitando varreduras por produto e permitindo ***lookup* O(1)**.

---

## üßæ Formatos/campos de sa√≠da

### `output_forecast/full_dataset_predictions.parquet` (solver)

* `internal_store_id` (ou `pdv`)
* `internal_product_id` (ou `produto`)
* `horizon` (1..5)
* `prediction` (float)

### `output_forecast/predictions_tmp/merged_predictions_int.parquet` (concat)

* `pdv` (string/id)
* `produto` (string/id)
* `semana` (1..5)
* `quantidade` (inteiro; `ROUND(prediction)`)

---

## üö¶ Checkpoints, retomada e limpeza

* Os **chunks** s√£o salvos em `predictions_tmp/chunk_*.parquet`.
* **Retomar**: por padr√£o, o `solver.py` s√≥ **pula** um chunk se **parquet e csv** existem.

  * Se n√£o usa CSVs de chunk, **ajuste a checagem** para considerar apenas o parquet ou n√£o rode o `rm_csvs.py` **antes** de retomar.
* `rm_csvs.py` limpa CSVs de chunk (√∫til quando voc√™ migrou para s√≥-parquet).
* `concat.py` n√£o depende do parquet final do solver; ele l√™ **direto** dos parquets de chunk.

---

## üß∞ Dicas de performance

* **IO**: usar disco NVMe ajuda muito na escrita dos parquets de chunk.
* **Mem√≥ria**:

  * Diminua `STORE_CHUNK_SIZE` se ver *spikes* de RAM;
  * Reduza `TOP_LGBM_PAIRS` / `MAX_LGBM_PANEL_ROWS` se o painel LGBM ficar grande.
* **CPU**: DuckDB paraleliza `COPY (...)`/`read_parquet(...)`.
* **Barra de progresso**: avan√ßa **ap√≥s cada loja**; se ficar ‚Äúparado no 0%‚Äù, geralmente √© a **primeira loja/chunk** levando mais tempo (ex.: cache frio). A vers√£o atual evita filtros N√óM e deve evoluir continuamente.

---

## üßØ Solu√ß√£o de problemas

* **‚ÄúTravou em 0%‚Äù**: confirme que est√° usando a vers√£o com **m√©dias pr√©-computadas** (lookup), n√£o varrendo `weekly_df` por produto dentro do loop da loja.
* **Erro do DuckDB no final**: verifique espa√ßo em disco para o parquet final; como alternativa, use o `concat.py` (que l√™ parquets por glob) para gerar o arquivo renomeado e com arredondamento.
* **LightGBM indispon√≠vel**: o script segue sem LGBM (apenas m√©dias e zeros). Instale `lightgbm` para habilitar o bloco top-pairs.
* **Duplica√ß√£o ao retomar**: se voc√™ **n√£o** grava CSVs e a checagem pede `parquet AND csv`, o solver **reprocessa**. Ajuste a checagem para `if chunk_parquet.exists():` somente.

---

## üîí Reprodutibilidade

* `SEED=42` em amostragens/modelos;
* Pr√©-processamento determin√≠stico;
* **Loga** contagens de lojas/produtos, n¬∫ de pares extintos, produtos representativos e m√©tricas baseline;
* Vers√µes de libs recomendadas:

  * `duckdb>=1.0.0`, `pandas>=2.2`, `numpy>=1.26`, `pyarrow>=15`, `tqdm>=4.66`, `lightgbm>=4.0` (opcional).

---

## ‚úÖ Checklist r√°pido

* [ ] Ajustei os **caminhos** dos 3 parquets de entrada?
* [ ] Defini `TOP_LGBM_PAIRS`, `MAX_LGBM_PANEL_ROWS`, `PRODUCT_SAMPLE_RATE`, `STORE_CHUNK_SIZE`?
* [ ] Quero **CSV final** do solver? (se n√£o, comente a parte do CSV em `finalize_concatenation`)
* [ ] Vou **retomar** a execu√ß√£o? (ent√£o garanta a checagem correta de exist√™ncia dos chunks)
* [ ] Preciso dos nomes finais/inteiros? (rodar `concat.py`)

---

**Pronto!** Agora √© s√≥ rodar:

```bash
python solver.py
python rm_csvs.py          # opcional
python concat.py
```

Se quiser, me pe√ßa uma vers√£o do `solver.py` j√° **sem CSVs em nenhum ponto** e com o **resume** baseado apenas em parquet.



